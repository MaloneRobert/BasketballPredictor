{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "For this journal, we have created all of our models. We only saved the one model that created the most accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn import neural_network\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read in all the dataframes that we need to train and test our model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1314 = pd.read_csv('./preprocessed/final/diff1314.csv', sep=',')\n",
    "diff1415 = pd.read_csv('./preprocessed/final/diff1415.csv', sep=',')\n",
    "diff1516 = pd.read_csv('./preprocessed/final/diff1516.csv', sep=',')\n",
    "diff1617 = pd.read_csv('./preprocessed/final/diff1617.csv', sep=',')\n",
    "\n",
    "diff1718 = pd.read_csv('./preprocessed/final/diff1718.csv', sep=',')\n",
    "\n",
    "diff_norm1314 = pd.read_csv('./preprocessed/final/diff_norm1314.csv', sep=',')\n",
    "diff_norm1415 = pd.read_csv('./preprocessed/final/diff_norm1415.csv', sep=',')\n",
    "diff_norm1516 = pd.read_csv('./preprocessed/final/diff_norm1516.csv', sep=',')\n",
    "diff_norm1617 = pd.read_csv('./preprocessed/final/diff_norm1617.csv', sep=',')\n",
    "\n",
    "diff_norm1718 = pd.read_csv('./preprocessed/final/diff_norm1718.csv', sep=',')\n",
    "\n",
    "win1314 = pd.read_csv('./preprocessed/final/winner1314.csv', sep=',')\n",
    "win1415 = pd.read_csv('./preprocessed/final/winner1415.csv', sep=',')\n",
    "win1516 = pd.read_csv('./preprocessed/final/winner1516.csv', sep=',')\n",
    "win1617 = pd.read_csv('./preprocessed/final/winner1617.csv', sep=',')\n",
    "win1718 = pd.read_csv('./preprocessed/final/winner1718.csv', sep =',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to delete two columns that do not have any purpose. Not sure as to why they remained in our dataframes, but we remove them for all the difference dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 45)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [diff1314, diff1415, diff1516, diff1617, diff_norm1314, diff_norm1415, diff_norm1516, diff_norm1617, diff1718, diff_norm1718]\n",
    "\n",
    "for j in arr:\n",
    "    del j['Unnamed: 0']\n",
    "    del j['1']\n",
    "    \n",
    "diff1314.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we append the normalized and non-normalized data into one list that the models can be trained with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = [diff1314, diff1415, diff1516, diff1617]\n",
    "arr3 = [diff_norm1314, diff_norm1415, diff_norm1516, diff_norm1617]\n",
    "data = []\n",
    "data_norm = []\n",
    "\n",
    "for df in arr2:\n",
    "    for row in df.iterrows():\n",
    "        index, stat = row\n",
    "        data.append(stat.tolist())\n",
    "        \n",
    "for df in arr3:\n",
    "    for row in df.iterrows():\n",
    "        index, stat = row\n",
    "        data_norm.append(stat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to delete a column that has no important data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del win1314[\"Unnamed: 0\"]\n",
    "del win1415[\"Unnamed: 0\"]\n",
    "del win1516[\"Unnamed: 0\"]\n",
    "del win1617[\"Unnamed: 0\"]\n",
    "del win1718[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we append the data that contains the winners for the past 4 seasons. We use this for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "\n",
    "arr2 = [win1314, win1415, win1516, win1617]\n",
    "\n",
    "for i in arr2:\n",
    "    for row in i.iterrows():\n",
    "        index, winner = row\n",
    "        y.append(winner.tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our first model, the most accurate model, that is saved. It is a MLP neural network with non-normalized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network.MLPClassifier(solver = 'lbfgs', random_state = 0, hidden_layer_sizes = [10,10])\n",
    "#model.fit(data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = './finalized_model_1.sav'\n",
    "\n",
    "#pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of running this notebook, we get the saved model so that we can predict from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_1 = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we predict for this season, below you can see what the winners, being team a or team b, are. We have a better visualization in the other notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = loaded_model_1.predict(diff1718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to visualize the actual winners. Note that the predicted is longer because it also predicts for the last three series as well. But notice that our predicts are the exact same for the first 12 matchups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robert\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "winners1718 = win1718['Winner'].reshape(-1, 1)\n",
    "\n",
    "actual_win = []\n",
    "for x in winners1718:\n",
    "    temp = x[0]\n",
    "    actual_win.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the same process, but use normalized data. Notice that there are more errors from our predicted array and the actual winners above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_norm = neural_network.MLPClassifier(solver = 'lbfgs', random_state = 0, hidden_layer_sizes = [10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=[10, 10], learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_norm.fit(data_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_norm = model_norm.predict(diff_norm1718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do a normalized and non-normalzied decision tree classifier. It is not as accurate as the MLP neural network. So we decided not to save the machine learning models ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "model_dt = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt.fit(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = model_dt.predict(diff1718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do the same process with normalized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_norm = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt_norm.fit(data_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt_norm = model_dt_norm.predict(diff_norm1718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
